{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install faker"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fjntdFF3Wh6I",
        "outputId": "53f75305-fbd4-461c-d3d3-85884c70d135"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faker\n",
            "  Downloading Faker-24.3.0-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.4 in /usr/local/lib/python3.10/dist-packages (from faker) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.4->faker) (1.16.0)\n",
            "Installing collected packages: faker\n",
            "Successfully installed faker-24.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from faker import Faker\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "import nltk\n",
        "import re\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CxWQPk2C3GYy",
        "outputId": "074a5460-dc40-4328-9f90-d58c3ca9816c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_documents(word, num_docs=5, max_length=100):\n",
        "    faker = Faker()\n",
        "    documents = []\n",
        "    for _ in range(num_docs):\n",
        "        # Generate random text with the word included\n",
        "        doc = faker.paragraph()\n",
        "        if len(doc) > max_length:\n",
        "            doc = doc[:max_length]\n",
        "        doc += ' ' + word\n",
        "        documents.append(doc)\n",
        "    return documents"
      ],
      "metadata": {
        "id": "i45UbCZL3Z8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word = \"technology\"\n",
        "documents = generate_documents(word)\n",
        "\n",
        "preprocessed_documents=[]\n",
        "for doc in documents:\n",
        "    doc = doc.lower()\n",
        "\n",
        "    # Remove punctuation\n",
        "    doc = re.sub(r'[^\\w\\s]', '', doc)\n",
        "\n",
        "    tokens = word_tokenize(doc)\n",
        "\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "    preprocessed_documents.append(' '.join(tokens))\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform(preprocessed_documents)\n",
        "\n",
        "# get words\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "# Print TFIDF for each document\n",
        "for i, doc in enumerate(documents):\n",
        "    print(f\"TFIDF for Document {i+1}:\")\n",
        "    feature_index = tfidf_matrix[i,:].nonzero()[1]\n",
        "    tfidf_scores = zip(feature_index, [tfidf_matrix[i, x] for x in feature_index])\n",
        "    for w, s in [(feature_names[i], s) for (i, s) in tfidf_scores]:\n",
        "        print(f\"{w}: {s}\")\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TvfQDHvSWbNn",
        "outputId": "a6ec605c-ce7d-4d62-fed1-f00e662c3239"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TFIDF for Document 1:\n",
            "technology: 0.12633109113985516\n",
            "adult: 0.26511998030695105\n",
            "computer: 0.26511998030695105\n",
            "former: 0.26511998030695105\n",
            "speak: 0.26511998030695105\n",
            "strategy: 0.26511998030695105\n",
            "bag: 0.26511998030695105\n",
            "course: 0.26511998030695105\n",
            "candidate: 0.26511998030695105\n",
            "west: 0.26511998030695105\n",
            "outside: 0.26511998030695105\n",
            "resource: 0.26511998030695105\n",
            "whole: 0.26511998030695105\n",
            "plan: 0.26511998030695105\n",
            "deep: 0.26511998030695105\n",
            "\n",
            "TFIDF for Document 2:\n",
            "movement: 0.2749592798183583\n",
            "brother: 0.2749592798183583\n",
            "talk: 0.2749592798183583\n",
            "purpose: 0.2749592798183583\n",
            "cold: 0.2749592798183583\n",
            "national: 0.2749592798183583\n",
            "end: 0.2749592798183583\n",
            "decision: 0.2749592798183583\n",
            "value: 0.2749592798183583\n",
            "dream: 0.2749592798183583\n",
            "assume: 0.2749592798183583\n",
            "doctor: 0.2749592798183583\n",
            "various: 0.2749592798183583\n",
            "technology: 0.13101957007640605\n",
            "\n",
            "TFIDF for Document 3:\n",
            "certain: 0.2859822148170311\n",
            "someone: 0.2859822148170311\n",
            "parent: 0.2859822148170311\n",
            "social: 0.2859822148170311\n",
            "according: 0.2859822148170311\n",
            "attack: 0.2859822148170311\n",
            "spend: 0.2859822148170311\n",
            "news: 0.2859822148170311\n",
            "never: 0.2859822148170311\n",
            "establish: 0.2859822148170311\n",
            "since: 0.2859822148170311\n",
            "floor: 0.2859822148170311\n",
            "technology: 0.13627205766460584\n",
            "\n",
            "TFIDF for Document 4:\n",
            "listen: 0.2859822148170311\n",
            "wall: 0.2859822148170311\n",
            "conference: 0.2859822148170311\n",
            "carry: 0.2859822148170311\n",
            "civil: 0.2859822148170311\n",
            "table: 0.2859822148170311\n",
            "letter: 0.2859822148170311\n",
            "network: 0.2859822148170311\n",
            "whether: 0.2859822148170311\n",
            "still: 0.2859822148170311\n",
            "smile: 0.2859822148170311\n",
            "relate: 0.2859822148170311\n",
            "technology: 0.13627205766460584\n",
            "\n",
            "TFIDF for Document 5:\n",
            "hundred: 0.31269767647770536\n",
            "stop: 0.31269767647770536\n",
            "around: 0.31269767647770536\n",
            "concern: 0.31269767647770536\n",
            "personal: 0.31269767647770536\n",
            "fish: 0.31269767647770536\n",
            "relationship: 0.31269767647770536\n",
            "light: 0.31269767647770536\n",
            "prepare: 0.31269767647770536\n",
            "company: 0.31269767647770536\n",
            "technology: 0.1490021182884428\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fMrqD10jFBV8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(doc):\n",
        "    doc = doc.lower()\n",
        "    doc = ''.join([char for char in doc if char not in string.punctuation])\n",
        "    tokens = word_tokenize(doc)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "V6ZPu6AS3BRL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_tf(word, document):\n",
        "    word_count = document.count(word)\n",
        "    total_words = len(document)\n",
        "    return word_count / total_words\n",
        "\n",
        "def calculate_idf(word, documents):\n",
        "    num_documents_with_word = np.sum([1 for doc in documents if word in doc])\n",
        "    total_documents = len(documents)\n",
        "    return math.log((1+total_documents) / (1 + num_documents_with_word)) + 1\n",
        "\n",
        "def calculate_tf_idf(word, document, documents):\n",
        "    tf = calculate_tf(word, document)\n",
        "    idf = calculate_idf(word, documents)\n",
        "    return tf * idf\n",
        "\n",
        "\n",
        "preprocessed_documents = [preprocess_text(doc) for doc in documents]\n",
        "\n",
        "tfidf_scores = []\n",
        "for doc in preprocessed_documents:\n",
        "    tfidf_doc = {}\n",
        "    for word in set(doc):\n",
        "        tfidf_doc[word] = calculate_tf_idf(word, doc, preprocessed_documents)\n",
        "    tfidf_scores.append(tfidf_doc)\n",
        "\n",
        "normalized_tfidf_scores = []\n",
        "for doc in tfidf_scores:\n",
        "    sum_of_squares = np.sum(score ** 2 for score in doc.values())\n",
        "    sqrt_sum_of_squares = math.sqrt(sum_of_squares)\n",
        "    normalized_doc = {word: score / sqrt_sum_of_squares for word, score in doc.items()}\n",
        "    normalized_tfidf_scores.append(normalized_doc)\n",
        "\n",
        "# Print TFIDF for each document\n",
        "for i, doc in enumerate(normalized_tfidf_scores):\n",
        "    print(f\"TFIDF for Document {i+1}:\")\n",
        "    for word, score in doc.items():\n",
        "        print(f\"{word}: {score}\")\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FmlM3ZGfugnj",
        "outputId": "3044db8c-11e3-4bb7-9190-9d359e8a3a26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TFIDF for Document 1:\n",
            "candidate: 0.2651199803069511\n",
            "strategy: 0.2651199803069511\n",
            "adult: 0.2651199803069511\n",
            "bag: 0.2651199803069511\n",
            "whole: 0.2651199803069511\n",
            "deep: 0.2651199803069511\n",
            "west: 0.2651199803069511\n",
            "outside: 0.2651199803069511\n",
            "former: 0.2651199803069511\n",
            "speak: 0.2651199803069511\n",
            "computer: 0.2651199803069511\n",
            "plan: 0.2651199803069511\n",
            "technology: 0.1263310911398552\n",
            "course: 0.2651199803069511\n",
            "resource: 0.2651199803069511\n",
            "\n",
            "TFIDF for Document 2:\n",
            "various: 0.2749592798183583\n",
            "talk: 0.2749592798183583\n",
            "cold: 0.2749592798183583\n",
            "decision: 0.2749592798183583\n",
            "movement: 0.2749592798183583\n",
            "technology: 0.13101957007640605\n",
            "purpose: 0.2749592798183583\n",
            "dream: 0.2749592798183583\n",
            "assume: 0.2749592798183583\n",
            "national: 0.2749592798183583\n",
            "end: 0.2749592798183583\n",
            "doctor: 0.2749592798183583\n",
            "value: 0.2749592798183583\n",
            "brother: 0.2749592798183583\n",
            "\n",
            "TFIDF for Document 3:\n",
            "never: 0.2749592798183583\n",
            "spend: 0.2749592798183583\n",
            "establish: 0.2749592798183583\n",
            "c: 0.2749592798183583\n",
            "according: 0.2749592798183583\n",
            "floor: 0.2749592798183583\n",
            "since: 0.2749592798183583\n",
            "someone: 0.2749592798183583\n",
            "parent: 0.2749592798183583\n",
            "social: 0.2749592798183583\n",
            "certain: 0.2749592798183583\n",
            "attack: 0.2749592798183583\n",
            "technology: 0.13101957007640605\n",
            "news: 0.2749592798183583\n",
            "\n",
            "TFIDF for Document 4:\n",
            "table: 0.2859822148170311\n",
            "whether: 0.2859822148170311\n",
            "still: 0.2859822148170311\n",
            "smile: 0.2859822148170311\n",
            "technology: 0.13627205766460584\n",
            "letter: 0.2859822148170311\n",
            "listen: 0.2859822148170311\n",
            "network: 0.2859822148170311\n",
            "relate: 0.2859822148170311\n",
            "conference: 0.2859822148170311\n",
            "civil: 0.2859822148170311\n",
            "carry: 0.2859822148170311\n",
            "wall: 0.2859822148170311\n",
            "\n",
            "TFIDF for Document 5:\n",
            "prepare: 0.3126976764777054\n",
            "stop: 0.3126976764777054\n",
            "personal: 0.3126976764777054\n",
            "around: 0.3126976764777054\n",
            "company: 0.3126976764777054\n",
            "fish: 0.3126976764777054\n",
            "hundred: 0.3126976764777054\n",
            "concern: 0.3126976764777054\n",
            "relationship: 0.3126976764777054\n",
            "technology: 0.1490021182884428\n",
            "light: 0.3126976764777054\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-ed911971efd2>:28: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
            "  sum_of_squares = np.sum(score ** 2 for score in doc.values())\n"
          ]
        }
      ]
    }
  ]
}